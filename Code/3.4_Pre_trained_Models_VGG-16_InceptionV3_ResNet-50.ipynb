{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGMMYNtyEUKA"
   },
   "source": [
    "### Setting up the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJDjYNJ2EQxV"
   },
   "source": [
    "Since we started with cats and dogs, let us take up the dataset of Cat and Dog Images. The original training dataset on Kaggle has 25000 images of cats and dogs and the test dataset has 10000 unlabelled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X18VMpbDstO",
    "outputId": "58face9c-b1a2-4fbc-878d-0c710a371762"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O /tmp/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uATbliNEZ5a"
   },
   "source": [
    "Import the basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SitgnsKmD2EY"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import zipfile \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import Model \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eQ4ROiSE4iO"
   },
   "source": [
    "### Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-onTAOEsE12-"
   },
   "source": [
    "We will first prepare the dataset and separate out the images:\n",
    "\n",
    "We first divide the folder contents into the train and validation directories.\n",
    "Then, in each of the directories, create a separate directory for cats that contains only cat images, and a separate director for dogs having only dog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sbxdYATMEff8"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/cats_and_dogs_filtered.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m local_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp/cats_and_dogs_filtered.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m zip_ref \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_zip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m zip_ref\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m zip_ref\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\zipfile.py:1248\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/cats_and_dogs_filtered.zip'"
     ]
    }
   ],
   "source": [
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyDWjm_jFYs2"
   },
   "source": [
    "Let us check if the images have been loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "id": "BRwPVdonE-68",
    "outputId": "af233610-1b6f-426b-aa9a-70ba061e8284"
   },
   "outputs": [],
   "source": [
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "import matplotlib.image as mpimg\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols*4, nrows*4)\n",
    "pic_index = 100\n",
    "train_cat_fnames = os.listdir( train_cats_dir )\n",
    "train_dog_fnames = os.listdir( train_dogs_dir )\n",
    "\n",
    "\n",
    "next_cat_pix = [os.path.join(train_cats_dir, fname) \n",
    "                for fname in train_cat_fnames[ pic_index-8:pic_index] \n",
    "               ]\n",
    "\n",
    "next_dog_pix = [os.path.join(train_dogs_dir, fname) \n",
    "                for fname in train_dog_fnames[ pic_index-8:pic_index]\n",
    "               ]\n",
    "\n",
    "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
    "  # Set up subplot; subplot indices start at 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off') # Don't show axes (or gridlines)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6eHvXyDFfEi"
   },
   "source": [
    "## Pre-Trained Models for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEDbjARaFhi3"
   },
   "source": [
    "### VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlQp-3rKF53F"
   },
   "source": [
    "Step 1: Image Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XHtodUHFyRA"
   },
   "source": [
    "Since we took up a much smaller dataset of images earlier, we can make up for it by augmenting this data and increasing our dataset size. If you are working with the original larger dataset, you can skip this step and move straight on to building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lD83ofcFTxS"
   },
   "outputs": [],
   "source": [
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZath7ObF2Re"
   },
   "source": [
    "Step 2: Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oV2haYYiFrWr",
    "outputId": "84b29b53-5617-444b-d399-9cd0383b5a7a"
   },
   "outputs": [],
   "source": [
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))\n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory( validation_dir,  batch_size = 20, class_mode = 'binary', target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk14QdcpGCfu"
   },
   "source": [
    "Step 3: Loading the Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y92hfj-TGAve"
   },
   "source": [
    "We will be using only the basic models, with changes made only to the final layer. This is because this is just a binary classification problem while these models are built to handle up to 1000 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvQ22IvIF9Ss",
    "outputId": "6b11ab62-1a78-454b-8428-a04a8a0dc024"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "base_model = VGG16(input_shape = (224, 224, 3), # Shape of our images\n",
    "include_top = False, # Leave out the last fully connected layer\n",
    "weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpfPsyFxGLPO"
   },
   "source": [
    "Since we don’t have to train all the layers, we make them non_trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTJID_HjGGHS"
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pe61cBSGSfO"
   },
   "source": [
    "Step 4: Compile and Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6WxobfRGQ4-"
   },
   "source": [
    "We will then build the last fully-connected layer. I have just used the basic settings, but feel free to experiment with different values of dropout, and different Optimisers and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZGVZyCtGNqU",
    "outputId": "b7144ab7-4dcd-4387-eea7-04269f4a2bc7"
   },
   "outputs": [],
   "source": [
    "# Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(base_model.output)\n",
    "\n",
    "# Add a fully connected layer with 512 hidden units and ReLU activation\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a dropout rate of 0.5\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Add a final sigmoid layer for classification\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001), loss = 'binary_crossentropy',metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dmj_n8kGbRH"
   },
   "source": [
    "We will now build the final model based on the training and validation sets we created earlier. Please note to use the original directories itself instead of the augmented datasets I have used below. I have used just 10 epochs, but you can also increase them to get better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "3rly-szJGWuV",
    "outputId": "21fe374e-d257-4a13-b9fe-0e53e5a684ab"
   },
   "outputs": [],
   "source": [
    "vgghist = model.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObZsVoORG02P"
   },
   "source": [
    "you can see, we were able to achieve a validation Accuracy of 93% with just 5 epochs and without any major changes to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyIfw0lLHL6O"
   },
   "source": [
    "### 2. Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9srg_JhIQWu"
   },
   "source": [
    "Step 1: Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vcty26J4IIT3"
   },
   "source": [
    "You will note that I am not performing extensive data augmentation. The code is the same as before. I have just changed the image dimensions for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8FupA-BGgUj"
   },
   "outputs": [],
   "source": [
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2,shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53Ez6Ln9INUv"
   },
   "source": [
    "Step 2: Training and Validation Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi1_wOU5Henf"
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (150, 150))\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, batch_size = 20, class_mode = 'binary', target_size = (150, 150))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDcakdp5ITcP"
   },
   "source": [
    "Step 3: Loading the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNCHR7CRHhCu"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "base_model = InceptionV3(input_shape = (150, 150, 3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Kdteq2EIXmG"
   },
   "source": [
    "Step 4: Compile and Fit\n",
    "Just like VGG-16, we will only change the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsFuktZEHkRf"
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDCPS5PeId8n"
   },
   "source": [
    "We perform the following operations:\n",
    "\n",
    "- Flatten the output of our base model to 1 dimension\n",
    "- Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
    "- This time, we will go with a dropout rate of 0.2\n",
    "- Add a final Fully Connected Sigmoid Layer\n",
    "- We will again use RMSProp, though you can try out the Adam Optimiser too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ftBIdPMHnLA"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znycKUX4Ir0G"
   },
   "source": [
    "We will then fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbG_EjZfHqLW"
   },
   "outputs": [],
   "source": [
    "inc_history = model.fit_generator(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1X_tli0UH1-m"
   },
   "source": [
    "We can see that we get 96% Validation accuracy in 5 epochs. Also note, how this model is much faster than VGG16. Each epoch is taking around only 1/4th the time that each epoch in VGG16. Of course, you can always experiment with the different hyperparameter values and see how much better/worse it performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GVoFRxiIzK7"
   },
   "source": [
    "### 3. ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhJTaBYJJFjn"
   },
   "source": [
    "Let us now use ResNet50 on our dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tnXr7uaJDGt"
   },
   "source": [
    "Step 1: Data Augmentation and Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljs1T34PH3oy"
   },
   "outputs": [],
   "source": [
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory( validation_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VepQFtziJI-e"
   },
   "source": [
    "Step 2: Import the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipvCfqk2JJbJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "base_model = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OppQMLX-JOSn"
   },
   "source": [
    "Again, we are using only the basic ResNet model, so we will keep the layers frozen and only modify the last layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDLclXzIJOwZ"
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFA91qvGJVAv"
   },
   "source": [
    "Step 3: Build and Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFw2NcjyJS5C"
   },
   "source": [
    "Here, I would like to show you an even shorter code for using the ResNet50 model. We will use this model just as a layer in a Sequential model, and just add a single Fully Connected Layer on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmGRT19BJUg6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "base_model = Sequential()\n",
    "base_model.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n",
    "base_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMM8cLlnJcER"
   },
   "source": [
    "We compile the model and this time let us try the SGD optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CFI30ChJcjr"
   },
   "outputs": [],
   "source": [
    "base_model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQdrUcDJJgWu"
   },
   "source": [
    "Step 4: Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qe1v4nDJhFq"
   },
   "outputs": [],
   "source": [
    "resnet_history = base_model.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev0eYW1nJxGX"
   },
   "source": [
    "You can see how well it performs on our dataset and this makes ResNet50 one of the most widely used Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prRbWMi4Jxph"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Pre-trained Models",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
